{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Live Voice AI Assistant with Memory (OpenAI)\n",
        "\n",
        "## Objective\n",
        "\n",
        "This notebook demonstrates how to build a real-time Voice AI Assistant that:\n",
        "\n",
        "- Records live audio from microphone\n",
        "- Converts Speech â†’ Text using Whisper\n",
        "- Maintains conversation memory\n",
        "- Generates contextual GPT responses\n",
        "- Supports continuous interaction\n",
        "\n",
        "---\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "Microphone  \n",
        "   â†“  \n",
        "Whisper (Speech-to-Text)  \n",
        "   â†“  \n",
        "Conversation Memory  \n",
        "   â†“  \n",
        "GPT Model  \n",
        "   â†“  \n",
        "Assistant Response  \n",
        "\n",
        "---\n",
        "\n",
        "## Models Used\n",
        "\n",
        "- `whisper-1` â†’ Speech Recognition\n",
        "- `gpt-4.1-nano` â†’ Fast conversational model\n",
        "\n",
        "This notebook simulates a mini production-level voice assistant."
      ],
      "metadata": {
        "id": "Ewdd6c63wYBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "LDO1YfsTweFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai soundfile"
      ],
      "metadata": {
        "id": "6SshslUfwcE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Secure API Setup"
      ],
      "metadata": {
        "id": "KmDvP5c3wiNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load API credentials securely from Colab\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "OPENAI_BASE_URL = userdata.get(\"OPENAI_BASE_URL\")\n",
        "\n",
        "# Import OpenAI SDK\n",
        "from openai import OpenAI\n",
        "\n",
        "# Initialize client\n",
        "client = OpenAI(\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    base_url=OPENAI_BASE_URL\n",
        ")\n",
        "\n",
        "print(\"OpenAI Client Initialized Successfully\")"
      ],
      "metadata": {
        "id": "xXnDeviiwkOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Initialize Conversation Memory\n",
        "\n",
        "We maintain a `memory` list that stores:\n",
        "\n",
        "- System instructions\n",
        "- User messages\n",
        "- Assistant responses\n",
        "\n",
        "This enables contextual, multi-turn conversation."
      ],
      "metadata": {
        "id": "zsFdNccVwqGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful voice assistant. Remember user information and respond contextually.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Memory Initialized\")"
      ],
      "metadata": {
        "id": "V8JwJNrKw4Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Record Live Audio from Microphone\n",
        "\n",
        "This function:\n",
        "\n",
        "- Uses JavaScript inside Colab\n",
        "- Accesses browser microphone\n",
        "- Records for a fixed duration\n",
        "- Saves audio as .wav file"
      ],
      "metadata": {
        "id": "QbfnIkxcw6wB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript, display\n",
        "from google.colab.output import eval_js\n",
        "import base64\n",
        "\n",
        "def record_audio(seconds=5, filename=\"recorded_audio.webm\"):\n",
        "\n",
        "    print(\"ðŸŽ¤ Recording... Speak clearly!\")\n",
        "\n",
        "    display(Javascript(\"\"\"\n",
        "    async function recordAudio(seconds) {\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({audio: true});\n",
        "      const recorder = new MediaRecorder(stream);\n",
        "      let chunks = [];\n",
        "      recorder.ondataavailable = e => chunks.push(e.data);\n",
        "      recorder.start();\n",
        "      await new Promise(resolve => setTimeout(resolve, seconds * 1000));\n",
        "      recorder.stop();\n",
        "      await new Promise(resolve => recorder.onstop = resolve);\n",
        "      const blob = new Blob(chunks, {type: 'audio/webm'});\n",
        "      const arrayBuffer = await blob.arrayBuffer();\n",
        "      return btoa(\n",
        "        new Uint8Array(arrayBuffer)\n",
        "          .reduce((data, byte) => data + String.fromCharCode(byte), '')\n",
        "      );\n",
        "    }\n",
        "    \"\"\"))\n",
        "\n",
        "    audio_base64 = eval_js(f\"recordAudio({seconds})\")\n",
        "    audio_bytes = base64.b64decode(audio_base64)\n",
        "\n",
        "    with open(filename, \"wb\") as f:\n",
        "        f.write(audio_bytes)\n",
        "\n",
        "    print(\"âœ… Recording Saved (webm format)\")\n",
        "    return filename"
      ],
      "metadata": {
        "id": "9XTm26law9f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Convert Speech to Text (Whisper)\n",
        "\n",
        "This function sends recorded audio to the Whisper model\n",
        "and returns the transcribed text."
      ],
      "metadata": {
        "id": "UsThtOeCxBm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def speech_to_text(audio_path):\n",
        "\n",
        "    with open(audio_path, \"rb\") as audio_file:\n",
        "\n",
        "        transcription = client.audio.transcriptions.create(\n",
        "            model=\"whisper-1\",\n",
        "            file=audio_file,\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "    return transcription.text.strip()"
      ],
      "metadata": {
        "id": "dIVtvtamxDK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Generate Assistant Response with Memory\n",
        "\n",
        "This function:\n",
        "\n",
        "1. Appends user message to memory\n",
        "2. Sends full memory to GPT\n",
        "3. Stores assistant reply\n",
        "4. Returns the response"
      ],
      "metadata": {
        "id": "fuFzMV04xFBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_assistant_response(user_text):\n",
        "\n",
        "    memory.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": user_text\n",
        "    })\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4.1-nano\",   # upgraded from nano â†’ more accurate\n",
        "        messages=memory,\n",
        "        temperature=0.3   # controlled creativity\n",
        "    )\n",
        "\n",
        "    assistant_text = response.choices[0].message.content.strip()\n",
        "\n",
        "    memory.append({\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": assistant_text\n",
        "    })\n",
        "\n",
        "    return assistant_text"
      ],
      "metadata": {
        "id": "mNTWP8_TxGlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Continuous Live Voice Interaction\n",
        "\n",
        "This loop:\n",
        "\n",
        "- Waits for user command\n",
        "- Records live audio\n",
        "- Converts to text\n",
        "- Generates contextual response\n",
        "- Continues until user exits"
      ],
      "metadata": {
        "id": "mEJwlrMxxIvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "\n",
        "    command = input(\"\\nType 'speak' to talk or 'quit' to exit: \")\n",
        "\n",
        "    if command.lower() == \"quit\":\n",
        "        print(\"Exiting Voice Assistant\")\n",
        "        break\n",
        "\n",
        "    if command.lower() == \"speak\":\n",
        "\n",
        "        audio_file = record_audio(seconds=5)\n",
        "\n",
        "        user_text = speech_to_text(audio_file)\n",
        "        print(\"\\n You said:\", user_text)\n",
        "\n",
        "        assistant_reply = get_assistant_response(user_text)\n",
        "        print(\"\\n Assistant:\", assistant_reply)"
      ],
      "metadata": {
        "id": "M7VNvFa1xKQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Observations\n",
        "\n",
        "## What This Notebook Achieves\n",
        "\n",
        "- Real-time microphone recording\n",
        "- Speech-to-text conversion using Whisper\n",
        "- Memory-based contextual conversation\n",
        "- Continuous voice interaction\n",
        "\n",
        "---\n",
        "\n",
        "## Key Learning Points\n",
        "\n",
        "1. Memory enables multi-turn intelligent dialogue\n",
        "2. Whisper accurately converts speech to text\n",
        "3. GPT models handle context effectively\n",
        "4. JavaScript enables microphone access in Colab\n",
        "\n",
        "---\n",
        "\n",
        "## Possible Improvements\n",
        "\n",
        "- Add Text-to-Speech (assistant speaks back)\n",
        "-  Add streaming GPT responses\n",
        "- Add long-term memory using database\n",
        "- Convert into Flask/FastAPI backend\n",
        "- Convert into AI Agent architecture\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This notebook demonstrates a production-style Voice AI assistant\n",
        "that listens, remembers, and responds intelligently in real-time."
      ],
      "metadata": {
        "id": "PR3g02aQxvq0"
      }
    }
  ]
}