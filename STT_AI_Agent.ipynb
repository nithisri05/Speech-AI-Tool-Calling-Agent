{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Speech-to-Text AI Agent with Tool Calling\n",
        "\n",
        "## Objective\n",
        "\n",
        "This notebook demonstrates how to build a speech-enabled AI agent that:\n",
        "\n",
        "- Converts audio input to text using Whisper\n",
        "- Uses GPT function calling (tool calling)\n",
        "- Dynamically invokes Python tools\n",
        "- Maintains conversation memory\n",
        "- Performs multi-step reasoning\n",
        "\n",
        "---\n",
        "\n",
        "## What This Notebook Covers\n",
        "\n",
        "1. Secure API initialization\n",
        "2. Conversation memory handling\n",
        "3. Tool schema design\n",
        "4. Function calling workflow\n",
        "5. Multi-step agent execution\n",
        "6. Real-time tool invocation"
      ],
      "metadata": {
        "id": "kWu0HBwi9332"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System Architecture\n",
        "\n",
        "Audio Input  \n",
        "   ↓  \n",
        "Whisper (Speech-to-Text)  \n",
        "   ↓  \n",
        "GPT Model  \n",
        "   ↓  \n",
        "Tool Decision (Function Calling)  \n",
        "   ↓  \n",
        "Execute Python Tool  \n",
        "   ↓  \n",
        "Final GPT Response  \n",
        "\n",
        "This represents an agentic AI workflow."
      ],
      "metadata": {
        "id": "zbzwtU-s98r9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install Dependencies"
      ],
      "metadata": {
        "id": "kVAsejRC-Fwz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxU3V0mL1CgP"
      },
      "outputs": [],
      "source": [
        "!pip install openai requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai requests"
      ],
      "metadata": {
        "id": "U3cWHwQA3_Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript, display\n",
        "from google.colab.output import eval_js\n",
        "import base64\n",
        "\n",
        "def record_audio(seconds=5, filename=\"live_audio.webm\"):\n",
        "\n",
        "    print(\"Recording... Speak now!\")\n",
        "\n",
        "    display(Javascript(\"\"\"\n",
        "    async function recordAudio(seconds) {\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({audio: true});\n",
        "      const recorder = new MediaRecorder(stream);\n",
        "      let chunks = [];\n",
        "      recorder.ondataavailable = e => chunks.push(e.data);\n",
        "      recorder.start();\n",
        "      await new Promise(resolve => setTimeout(resolve, seconds * 1000));\n",
        "      recorder.stop();\n",
        "      await new Promise(resolve => recorder.onstop = resolve);\n",
        "      const blob = new Blob(chunks, {type: 'audio/webm'});\n",
        "      const arrayBuffer = await blob.arrayBuffer();\n",
        "      return btoa(\n",
        "        new Uint8Array(arrayBuffer)\n",
        "          .reduce((data, byte) => data + String.fromCharCode(byte), '')\n",
        "      );\n",
        "    }\n",
        "    \"\"\"))\n",
        "\n",
        "    audio_base64 = eval_js(f\"recordAudio({seconds})\")\n",
        "    audio_bytes = base64.b64decode(audio_base64)\n",
        "\n",
        "    with open(filename, \"wb\") as f:\n",
        "        f.write(audio_bytes)\n",
        "\n",
        "    print(\"Recording complete\")\n",
        "    return filename"
      ],
      "metadata": {
        "id": "DrBOAU3l77t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API Setup\n",
        "\n",
        "We securely load API keys using Google Colab's secret storage.\n",
        "This prevents hardcoding credentials and improves security."
      ],
      "metadata": {
        "id": "kC4x9wLS-LsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import json\n",
        "import requests\n",
        "\n",
        "OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n",
        "OPENAI_BASE_URL = userdata.get(\"OPENAI_BASE_URL\")\n",
        "\n",
        "client = OpenAI(\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    base_url=OPENAI_BASE_URL\n",
        ")\n",
        "\n",
        "print(\"OpenAI Client Initialized\")"
      ],
      "metadata": {
        "id": "D96i_raj1R6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversation Memory\n",
        "\n",
        "We use structured memory with roles:\n",
        "\n",
        "- system → sets behavior\n",
        "- user → user inputs\n",
        "- assistant → model responses\n",
        "- tool → tool outputs\n",
        "\n",
        "This enables contextual multi-turn interaction."
      ],
      "metadata": {
        "id": "vBi6oay21YhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memory = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a voice assistant. Use tools when needed.\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "ukitFIh-1btX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tool Definitions\n",
        "\n",
        "We define real Python functions and expose them to GPT\n",
        "through tool schemas."
      ],
      "metadata": {
        "id": "1TfCeyQp1f1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------\n",
        "# Tool 1: Get Time\n",
        "# -------------------\n",
        "def get_time():\n",
        "    from datetime import datetime\n",
        "    import pytz\n",
        "\n",
        "    ist = pytz.timezone(\"Asia/Kolkata\")\n",
        "    current_time = datetime.now(ist)\n",
        "\n",
        "    return current_time.strftime(\"%I:%M %p\")\n",
        "# -------------------\n",
        "# Tool 2: Get Weather\n",
        "# -------------------\n",
        "WEATHER_API_KEY = userdata.get(\"WEATHER_API_KEY\")\n",
        "\n",
        "def get_weather(city):\n",
        "    url = \"http://api.weatherapi.com/v1/current.json\"\n",
        "\n",
        "    params = {\n",
        "        \"key\": WEATHER_API_KEY,\n",
        "        \"q\": city\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, params=params)\n",
        "    data = response.json()\n",
        "\n",
        "    location = data[\"location\"][\"name\"]\n",
        "    country = data[\"location\"][\"country\"]\n",
        "    temp = data[\"current\"][\"temp_c\"]\n",
        "    condition = data[\"current\"][\"condition\"][\"text\"]\n",
        "\n",
        "    return f\"The current weather in {location}, {country} is {temp}°C with {condition}.\""
      ],
      "metadata": {
        "id": "K3fsQuDV1hiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool_functions = {\n",
        "    \"get_time\": get_time,\n",
        "    \"get_weather\": get_weather\n",
        "}"
      ],
      "metadata": {
        "id": "6VhkA-xI2sLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_time\",\n",
        "            \"description\": \"Get current time\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {}\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_weather\",\n",
        "            \"description\": \"Get current weather for a city\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"city\": {\"type\": \"string\"}\n",
        "                },\n",
        "                \"required\": [\"city\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "uCHbZCTO2tNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speech-to-Text (Whisper)\n",
        "\n",
        "We convert audio input into text before passing it to GPT."
      ],
      "metadata": {
        "id": "nEtkk_Sl2vdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def speech_to_text(audio_path):\n",
        "\n",
        "    with open(audio_path, \"rb\") as audio_file:\n",
        "\n",
        "        transcription = client.audio.transcriptions.create(\n",
        "            model=\"whisper-1\",\n",
        "            file=audio_file,\n",
        "            temperature=0\n",
        "        )\n",
        "\n",
        "    return transcription.text.strip()"
      ],
      "metadata": {
        "id": "F56xq6oS2xj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Execution Logic\n",
        "\n",
        "1. Add user message to memory\n",
        "2. Call GPT with tool definitions\n",
        "3. If GPT requests a tool:\n",
        "   - Execute tool\n",
        "   - Add tool result to memory\n",
        "   - Call GPT again\n",
        "4. Return final response"
      ],
      "metadata": {
        "id": "LY905NvS2zey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_agent(user_text):\n",
        "\n",
        "    memory.append({\"role\": \"user\", \"content\": user_text})\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4.1-nano\",\n",
        "        messages=memory,\n",
        "        tools=tools,\n",
        "        tool_choice=\"auto\",\n",
        "        temperature=0.3\n",
        "    )\n",
        "\n",
        "    message = response.choices[0].message\n",
        "\n",
        "    # If tool is called\n",
        "    if message.tool_calls:\n",
        "\n",
        "        memory.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": None,\n",
        "            \"tool_calls\": message.tool_calls\n",
        "        })\n",
        "\n",
        "        tool_call = message.tool_calls[0]\n",
        "        tool_name = tool_call.function.name\n",
        "        args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "        print(\"Tool Called:\", tool_name)\n",
        "\n",
        "        result = tool_functions[tool_name](**args)\n",
        "\n",
        "        memory.append({\n",
        "            \"role\": \"tool\",\n",
        "            \"tool_call_id\": tool_call.id,\n",
        "            \"content\": result\n",
        "        })\n",
        "\n",
        "        second_response = client.chat.completions.create(\n",
        "            model=\"gpt-4.1-nano\",\n",
        "            messages=memory\n",
        "        )\n",
        "\n",
        "        final_text = second_response.choices[0].message.content\n",
        "\n",
        "    else:\n",
        "        final_text = message.content\n",
        "\n",
        "    memory.append({\"role\": \"assistant\", \"content\": final_text})\n",
        "\n",
        "    return final_text"
      ],
      "metadata": {
        "id": "hRryHrxm22MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "\n",
        "    command = input(\"\\nType 'speak' to talk or 'quit' to exit: \")\n",
        "\n",
        "    if command.lower() == \"quit\":\n",
        "        print(\"Exiting Agent\")\n",
        "        break\n",
        "\n",
        "    if command.lower() == \"speak\":\n",
        "\n",
        "        audio_path = record_audio(seconds=5)\n",
        "\n",
        "        user_text = speech_to_text(audio_path)\n",
        "        print(\"User:\", user_text)\n",
        "\n",
        "        response = run_agent(user_text)\n",
        "        print(\"Assistant:\", response)"
      ],
      "metadata": {
        "id": "IZs_rRH524yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Observations\n",
        "\n",
        "##  What This Notebook Demonstrates\n",
        "\n",
        "- Speech recognition using Whisper\n",
        "- GPT function calling\n",
        "- Dynamic tool execution\n",
        "- Multi-step reasoning\n",
        "- Memory-based conversation\n",
        "- Agentic AI workflow\n",
        "\n",
        "---\n",
        "\n",
        "## Key Learning Outcomes\n",
        "\n",
        "1. LLMs can extend capabilities using tools\n",
        "2. Function schemas guide tool usage\n",
        "3. Memory enables contextual dialogue\n",
        "4. Agents require multi-step execution logic\n",
        "\n",
        "---\n",
        "\n",
        "## Possible Enhancements\n",
        "\n",
        "- Add more tools (calculator, database, search)\n",
        "- Add text-to-speech output\n",
        "- Deploy using FastAPI\n",
        "- Add long-term persistent memory"
      ],
      "metadata": {
        "id": "9vDGYtO5-vVK"
      }
    }
  ]
}